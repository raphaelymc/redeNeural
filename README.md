# Projeto: Implementa√ß√£o de Rede Neural Artificial (RNA) para Classifica√ß√£o Bin√°ria

## üìù Descri√ß√£o
Este projeto √© uma implementa√ß√£o fundamental de uma Rede Neural Artificial (RNA), desenvolvida do zero em Python como parte dos estudos em Fundamentos de Intelig√™ncia Artificial. O objetivo foi simular o aprendizado e demonstrar a compreens√£o pr√°tica do mecanismo de ajuste de pesos (sinapses) por meio da retropropaga√ß√£o. A rede foi configurada para resolver um problema b√°sico de classifica√ß√£o bin√°ria.

## üíª Stack Tecnol√≥gico
* **Linguagem:** Python
* **Biblioteca NumPy:** Essencial para lidar com todas as opera√ß√µes matriciais necess√°rias, como o produto interno no *feedforward* e o c√°lculo do gradiente no *backpropagation*.
* **Biblioteca Matplotlib:** Utilizada para visualiza√ß√£o e registro da converg√™ncia do erro durante o treinamento.
* **Ambiente:** Desenvolvido no Visual Studio Code (VS Code).

## ‚öôÔ∏è Arquitetura e Metodologia
A arquitetura consiste em uma rede simples de uma √∫nica camada:

* **Fun√ß√£o de Ativa√ß√£o:** Utiliza√ß√£o da **Fun√ß√£o Sigmoide** e sua derivada. A Sigmoide comprime a sa√≠da entre 0 e 1, ideal para classifica√ß√£o de probabilidade.
* **Treinamento (Backpropagation):** O loop de treinamento foi executado por **10.000 √©pocas**. Em cada itera√ß√£o, a rede realizou a propaga√ß√£o para frente (*feedforward*) e utilizou o erro para retropropagar e calcular o ajuste necess√°rio nos pesos (*backpropagation*).

## üìä Resultados e An√°lise

### 1. Converg√™ncia do Erro
A converg√™ncia do erro √© a evid√™ncia mais forte do aprendizado. O erro m√©dio, que se inicia alto, caiu drasticamente nas primeiras √©pocas, estabilizando-se em um valor muito pr√≥ximo de zero. Isso demonstra que a rede convergiu de forma r√°pida e est√°vel.

### 2. Sa√≠da Final e Precis√£o
A sa√≠da final confirma a precis√£o da classifica√ß√£o. Para as entradas onde a resposta esperada era 0, a rede produziu valores como **0.01**, e para as entradas esperadas como 1, produziu valores de alta confian√ßa (**0.98** e **0.89**).

## üîë Conclus√£o do Estudo
O exerc√≠cio pr√°tico atingiu seu objetivo, fornecendo uma base s√≥lida sobre o mecanismo de aprendizado. Observar o erro caindo e os pesos se ajustando refor√ßa a compreens√£o de que o aprendizado de m√°quina √©, essencialmente, um processo de otimiza√ß√£o matem√°tica. Este projeto serve como uma base s√≥lida para o desenvolvimento de redes neurais mais complexas no futuro.
